{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import node2vec\n",
    "\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIDS Data\n",
    "https://networkrepository.com/AIDS.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIDS_EDGES = None\n",
    "AIDS_LABELS = None\n",
    "\n",
    "with open('data/AIDS/AIDS.edges.txt') as edges_file:\n",
    "    AIDS_EDGES = np.loadtxt(edges_file, dtype=int, delimiter=',')\n",
    "\n",
    "with open('data/AIDS/AIDS.node_labels.txt') as nodes_file:\n",
    "    AIDS_LABELS = np.loadtxt(nodes_file, dtype=int, delimiter=',')\n",
    "    AIDS_LABELS = AIDS_LABELS[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FB CMU Data\n",
    "https://networkrepository.com/fb-CMU-Carnegie49.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_CMU_EDGES = None\n",
    "FB_CMU_LABELS = None\n",
    "\n",
    "with open('data/fb-CMU-Carnegie49/fb-CMU-Carnegie49.edges.txt') as edges_file:\n",
    "    FB_CMU_EDGES = np.loadtxt(edges_file, dtype=int, delimiter=' ')\n",
    "\n",
    "with open('data/fb-CMU-Carnegie49/fb-CMU-Carnegie49.node_labels.txt') as nodes_file:\n",
    "    FB_CMU_LABELS = np.loadtxt(nodes_file, dtype=int, delimiter=',')\n",
    "    FB_CMU_LABELS = FB_CMU_LABELS[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Spam Detection Data\n",
    "https://networkrepository.com/web-spam-detection.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPAM_EDGES = None\n",
    "SPAM_LABELS = None\n",
    "\n",
    "with open('data/web-spam-detection/web-spam-detection.edges.txt') as edges_file:\n",
    "    SPAM_EDGES = np.loadtxt(edges_file, dtype=int, delimiter=',')\n",
    "\n",
    "with open('data/web-spam-detection/web-spam-detection.node_labels.txt') as nodes_file:\n",
    "    SPAM_LABELS = np.loadtxt(nodes_file, dtype=int, delimiter=',')\n",
    "    SPAM_LABELS = SPAM_LABELS[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = AIDS_EDGES\n",
    "# Y = AIDS_LABELS\n",
    "\n",
    "X = FB_CMU_EDGES\n",
    "Y = FB_CMU_LABELS\n",
    "\n",
    "# X = SPAM_EDGES\n",
    "# Y = SPAM_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "NUM_NODES = np.max(X)\n",
    "NUM_CLASSES_CLUSTERS = np.max(Y)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tg.data.Data(\n",
    "    x=torch.eye(NUM_NODES), \n",
    "    edge_index=torch.tensor(X-1, dtype=torch.long).t().contiguous(), \n",
    "    y=torch.nn.functional.one_hot(torch.tensor(Y-1, dtype=torch.long), NUM_CLASSES_CLUSTERS)).to(device)\n",
    "    # y=torch.tensor(Y-1, dtype=torch.long)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.zeros((NUM_NODES, NUM_NODES))\n",
    "adjacency_matrix[X[:, 0]-1, X[:, 1]-1] = 1\n",
    "adjacency_matrix[X[:, 1]-1, X[:, 0]-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_nodes, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = torch.nn.functional.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fcn = FCN(NUM_NODES, NUM_CLASSES_CLUSTERS).to(device)\n",
    "optimizer_fcn = torch.optim.Adam(model_fcn.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "loss_fn_fcn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(adjacency_matrix, Y, test_size=0.2, random_state=42)\n",
    "X_TRAIN = torch.tensor(X_TRAIN, dtype=torch.float).to(device)\n",
    "X_TEST = torch.tensor(X_TEST, dtype=torch.float).to(device)\n",
    "Y_TRAIN = torch.nn.functional.one_hot(torch.tensor(Y_TRAIN-1, dtype=torch.long)).to(device)\n",
    "Y_TEST = torch.nn.functional.one_hot(torch.tensor(Y_TEST-1, dtype=torch.long)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_fcn = []\n",
    "acc_list_fcn = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_fcn.train()\n",
    "    optimizer_fcn.zero_grad()\n",
    "    out = model_fcn(X_TRAIN)\n",
    "    loss = loss_fn_fcn(out, Y_TRAIN.float())\n",
    "    loss.backward()\n",
    "    optimizer_fcn.step()\n",
    "\n",
    "    model_fcn.eval()\n",
    "    _, pred = model_fcn(X_TEST).max(dim=1)\n",
    "    correct = float(pred.eq(Y_TEST.float().argmax(dim=1)).sum().item())\n",
    "    acc = correct / len(Y_TEST)\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print('Epoch: {:03d}, Loss: {:.5f}, Acc: {:.5f}'.format(epoch+1, loss, acc))\n",
    "\n",
    "    loss_list_fcn.append(loss.item())\n",
    "    acc_list_fcn.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "model_fcn.eval()\n",
    "_, pred = model_fcn(X_TEST).max(dim=1)\n",
    "correct = float(pred.eq(Y_TEST.argmax(dim=1)).sum().item())\n",
    "acc = correct / len(data.y)\n",
    "print('Test Accuracy: {:.5f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy\n",
    "plt.plot(loss_list_fcn)\n",
    "plt.plot(acc_list_fcn)\n",
    "plt.legend(['loss', 'accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = torch.nn.Linear(128*10*10, 1024)\n",
    "        self.fc2 = torch.nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128*10*10)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = CNN(NUM_NODES, NUM_CLASSES_CLUSTERS).to(device)\n",
    "optimizer_cnn = torch.optim.Adam(model_cnn.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "loss_fn_cnn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(adjacency_matrix, Y, test_size=0.2, random_state=42)\n",
    "X_TRAIN = torch.tensor(X_TRAIN, dtype=torch.float).to(device)\n",
    "X_TEST = torch.tensor(X_TEST, dtype=torch.float).to(device)\n",
    "Y_TRAIN = torch.nn.functional.one_hot(torch.tensor(Y_TRAIN-1, dtype=torch.long)).to(device)\n",
    "Y_TEST = torch.nn.functional.one_hot(torch.tensor(Y_TEST-1, dtype=torch.long)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_TRAIN.view(-1, 1, 49, 49)\n",
    "X_TEST = X_TEST.view(-1, 1, 49, 49)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_TRAIN, Y_TRAIN)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for data, label in dataloader:\n",
    "        model_cnn.train()\n",
    "        optimizer_cnn.zero_grad()\n",
    "        out = model_cnn(data)\n",
    "        loss = loss_fn_cnn(out, label.float())\n",
    "        loss.backward()\n",
    "        optimizer_cnn.step()\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}'.format(epoch+1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_cnn = []\n",
    "acc_list_cnn = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_cnn.train()\n",
    "    optimizer_cnn.zero_grad()\n",
    "    out = model_cnn(X_TRAIN)\n",
    "    loss = loss_fn_fcn(out, Y_TRAIN.float())\n",
    "    loss.backward()\n",
    "    optimizer_cnn.step()\n",
    "\n",
    "    model_cnn.eval()\n",
    "    _, pred = model_cnn(X_TEST).max(dim=1)\n",
    "    correct = float(pred.eq(Y_TEST.float().argmax(dim=1)).sum().item())\n",
    "    acc = correct / len(Y_TEST)\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print('Epoch: {:03d}, Loss: {:.5f}, Acc: {:.5f}'.format(epoch+1, loss, acc))\n",
    "\n",
    "    loss_list_fcn.append(loss.item())\n",
    "    acc_list_fcn.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "model_cnn.eval()\n",
    "_, pred = model_cnn(X_TEST).max(dim=1)\n",
    "correct = float(pred.eq(Y_TEST.argmax(dim=1)).sum().item())\n",
    "acc = correct / len(data.y)\n",
    "print('Test Accuracy: {:.5f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy\n",
    "plt.plot(loss_list_cnn)\n",
    "plt.plot(acc_list_cnn)\n",
    "plt.legend(['loss', 'accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network with 2 fully connected layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_node_features, 1024)\n",
    "        self.conv2 = tg.nn.GCNConv(1024, 256)\n",
    "        self.conv3 = tg.nn.GCNConv(256, 128)\n",
    "        self.fc1 = torch.nn.Linear(128, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gcn = GCN(num_node_features=NUM_NODES, num_classes=NUM_CLASSES_CLUSTERS).to(device)\n",
    "optimizer_gcn = torch.optim.Adam(model_gcn.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "loss_fn_gcn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_gcn = []\n",
    "accuracy_list_gcn = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_gcn.train()\n",
    "    optimizer_gcn.zero_grad()\n",
    "    out = model_gcn(data)\n",
    "    loss = loss_fn_gcn(out, data.y.float())\n",
    "    loss.backward()\n",
    "    optimizer_gcn.step()\n",
    "\n",
    "    # validate the model\n",
    "    model_gcn.eval()\n",
    "    _, pred = model_gcn(data).max(dim=1)\n",
    "    _, true_labels = torch.max(data.y.float(), dim=1)\n",
    "    correct = float(pred.eq(true_labels).sum().item())\n",
    "    acc = correct / len(data.y)\n",
    "    loss_list_gcn.append(loss.item())\n",
    "    accuracy_list_gcn.append(acc)\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1:03d}\\t Loss: {loss:.4f}\\t Val Acc: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "model_gcn.eval()\n",
    "_, pred = model_gcn(data).max(dim=1)\n",
    "_, true_labels = torch.max(data.y.float(), dim=1)\n",
    "correct = float(pred.eq(true_labels).sum().item())\n",
    "acc = correct / len(data.y)\n",
    "print(f'Test Acc: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy\n",
    "plt.plot(loss_list_gcn)\n",
    "plt.plot(accuracy_list_gcn)\n",
    "plt.legend(['loss', 'accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Encoder Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoderEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GraphEncoderEmbedding, self).__init__()\n",
    "        self.conv1 = tg.nn.GCNConv(num_features, 256)\n",
    "        self.conv2 = tg.nn.GCNConv(256, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LDA, self).__init__()\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        class_labels = torch.unique(y)\n",
    "        mean_vectors = [torch.mean(X[y == cl], dim=0) for cl in class_labels]\n",
    "        S_W = torch.zeros((n_features, n_features))\n",
    "        for cl, mean_vec in zip(class_labels, mean_vectors):\n",
    "            class_sc_mat = torch.zeros((n_features, n_features))\n",
    "            for row in X[y == cl]:\n",
    "                row, mean_vec = row.reshape(n_features, 1), mean_vec.reshape(n_features, 1)\n",
    "                class_sc_mat += (row - mean_vec).dot((row - mean_vec).T)\n",
    "            S_W += class_sc_mat\n",
    "        overall_mean = torch.mean(X, dim=0)\n",
    "        S_B = torch.zeros((n_features, n_features))\n",
    "        for mean_vec in mean_vectors:\n",
    "            n = X[y == cl].shape[0]\n",
    "            mean_vec = mean_vec.reshape(n_features, 1)\n",
    "            overall_mean = overall_mean.reshape(n_features, 1)\n",
    "            S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "        eig_vals, eig_vecs = torch.eig(torch.inverse(S_W).matmul(S_B), eigenvectors=True)\n",
    "        eig_pairs = [(torch.abs(eig_vals[i, 0]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n",
    "        eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "        W = torch.stack([eig_pairs[i][1] for i in range(len(class_labels) - 1)], dim=1)\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphEncoderEmbedding(num_features=NUM_NODES, num_classes=NUM_CLASSES_CLUSTERS).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out, data.y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validate the model\n",
    "    model.eval()\n",
    "    _, pred = model(data.x, data.edge_index).max(dim=1)\n",
    "    _, true_labels = torch.max(data.y.float(), dim=1)\n",
    "    correct = float(pred.eq(true_labels).sum().item())\n",
    "    acc = correct / len(data.y)\n",
    "    loss_list.append(loss.item())\n",
    "    accuracy_list.append(acc)\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1:03d}\\t Loss: {loss:.4f}\\t Val Acc: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "_, true_labels = torch.max(data.y.float(), dim=1)\n",
    "correct = float(pred.eq(true_labels).sum().item())\n",
    "acc = correct / len(data.y)\n",
    "print(f'Test Acc: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy\n",
    "plt.plot(loss_list)\n",
    "plt.plot(accuracy_list)\n",
    "plt.legend(['loss', 'accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = SpectralEmbedding(n_components=NUM_NODES-10)  # 2D embedding\n",
    "spectral_embedding = embedding.fit_transform(adjacency_matrix, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(spectral_embedding, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_spectral = LinearDiscriminantAnalysis()\n",
    "lda_spectral.fit(X_TRAIN, Y_TRAIN)\n",
    "Y_PRED_LDA_SPECTRAL = lda_spectral.predict(X_TRAIN)\n",
    "\n",
    "print(\"Training Accuracy LDA:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TRAIN == Y_PRED_LDA_SPECTRAL) / len(Y_TRAIN)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TRAIN, Y_PRED_LDA_SPECTRAL)}\")\n",
    "\n",
    "print()\n",
    "print(\"Testing Accuracy LDA:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TEST == lda_spectral.predict(X_TEST)) / len(Y_TEST)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TEST, lda_spectral.predict(X_TEST))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_spectral = QuadraticDiscriminantAnalysis()\n",
    "qda_spectral.fit(X_TRAIN, Y_TRAIN)\n",
    "Y_PRED_QDA_SPECTRAL = qda_spectral.predict(X_TRAIN)\n",
    "\n",
    "print(\"Training Accuracy QDA:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TRAIN == Y_PRED_QDA_SPECTRAL) / len(Y_TRAIN)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TRAIN, Y_PRED_QDA_SPECTRAL)}\")\n",
    "\n",
    "print()\n",
    "print(\"Testing Accuracy QDA:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TEST == qda_spectral.predict(X_TEST)) / len(Y_TEST)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TEST, qda_spectral.predict(X_TEST))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_spectral = KMeans(n_clusters=NUM_CLASSES_CLUSTERS)\n",
    "kmeans_spectral.fit(X_TRAIN)\n",
    "Y_PRED_KMEANS_SPECTRAL = kmeans_spectral.predict(X_TRAIN)\n",
    "\n",
    "print(\"Training Accuracy KMeans:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TRAIN-1 == Y_PRED_KMEANS_SPECTRAL) / len(Y_TRAIN)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TRAIN-1, Y_PRED_KMEANS_SPECTRAL)}\")\n",
    "\n",
    "print()\n",
    "print(\"Testing Accuracy KMeans:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TEST-1 == kmeans_spectral.predict(X_TEST)) / len(Y_TEST)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TEST-1, kmeans_spectral.predict(X_TEST))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_spectral = SVC()\n",
    "svm_spectral.fit(X_TRAIN, Y_TRAIN)\n",
    "Y_PRED_SVM_SPECTRAL = svm_spectral.predict(X_TRAIN)\n",
    "\n",
    "print(\"Training Accuracy SVM:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TRAIN == Y_PRED_SVM_SPECTRAL) / len(Y_TRAIN)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TRAIN, Y_PRED_SVM_SPECTRAL)}\")\n",
    "\n",
    "print()\n",
    "print(\"Testing Accuracy SVM:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TEST == svm_spectral.predict(X_TEST)) / len(Y_TEST)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TEST, svm_spectral.predict(X_TEST))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_model = node2vec.Node2Vec(G, dimensions=NUM_NODES-10, num_walks=200, workers=4)\n",
    "\n",
    "model = node2vec_model.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "embeddings = model.wv\n",
    "\n",
    "node_ids = list(G.nodes())  # List of nodes in the graph\n",
    "embedding_matrix = np.array([model.wv[str(node_id)] for node_id in node_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(embedding_matrix, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_node = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda_node.fit(X_TRAIN, Y_TRAIN)\n",
    "Y_PRED_LDA_NODE = lda_node.predict(X_TRAIN)\n",
    "\n",
    "print(\"Training Accuracy LDA:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TRAIN == Y_PRED_LDA_NODE) / len(Y)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TRAIN, Y_PRED_LDA_NODE)}\")\\\n",
    "\n",
    "print()\n",
    "print(\"Testing Accuracy LDA:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TEST == lda_node.predict(X_TEST)) / len(Y_TEST)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TEST, lda_node.predict(X_TEST))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_node = QuadraticDiscriminantAnalysis()\n",
    "qda_node.fit(X_TRAIN, Y_TRAIN)\n",
    "Y_PRED_QDA_NODE = qda_node.predict(X_TRAIN)\n",
    "\n",
    "print(\"Training Accuracy QDA:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TRAIN == Y_PRED_QDA_NODE) / len(Y_TRAIN)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TRAIN, Y_PRED_QDA_NODE)}\")\n",
    "\n",
    "print()\n",
    "print(\"Testing Accuracy QDA:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TEST == qda_node.predict(X_TEST)) / len(Y_TEST)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TEST, qda_node.predict(X_TEST))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_node = KMeans(n_clusters=NUM_CLASSES_CLUSTERS)\n",
    "kmeans_node.fit(X_TRAIN)\n",
    "Y_PRED_KMEANS_NODE = kmeans_node.predict(X_TRAIN)\n",
    "\n",
    "print(\"Training Accuracy KMeans:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TRAIN-1 == Y_PRED_KMEANS_NODE) / len(Y_TRAIN)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TRAIN-1, Y_PRED_KMEANS_NODE)}\")\n",
    "\n",
    "print()\n",
    "print(\"Testing Accuracy KMeans:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TEST-1 == kmeans_node.predict(X_TEST)) / len(Y_TEST)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TEST-1, kmeans_node.predict(X_TEST))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_node = SVC()\n",
    "svm_node.fit(X_TRAIN, Y_TRAIN)\n",
    "Y_PRED_SVM_NODE = svm_node.predict(X_TRAIN)\n",
    "\n",
    "print(\"Training Accuracy SVM:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TRAIN == Y_PRED_SVM_NODE) / len(Y_TRAIN)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TRAIN, Y_PRED_SVM_NODE)}\")\n",
    "\n",
    "print()\n",
    "print(\"Testing Accuracy SVM:\")\n",
    "print(f\"Accuracy: {np.sum(Y_TEST == svm_node.predict(X_TEST)) / len(Y_TEST)}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_score(Y_TEST, svm_node.predict(X_TEST))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
